{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283d18f0",
   "metadata": {},
   "source": [
    "\n",
    "# RAG Demo â€” Embeddings + FAISS retrieval (small, runnable notebook)\n",
    "\n",
    "**Purpose:** Demonstrates a minimal Retrieval-Augmented Generation (RAG) flow:\n",
    "1. Create embeddings for a small set of documents using `sentence-transformers`.\n",
    "2. Index embeddings with `faiss`.\n",
    "3. Perform a simple retrieval for a user query.\n",
    "4. (Optional) Show how to plug retrieved context into a prompt for an LLM.\n",
    "\n",
    "**How to run:**  \n",
    "- This notebook can be run locally or in Google Colab.  \n",
    "- If running locally, create a virtualenv and install the required packages:\n",
    "```\n",
    "pip install -U sentence-transformers faiss-cpu numpy\n",
    "```\n",
    "- If you want to use an LLM provider (e.g., OpenAI), you'll need to add your API key and follow their usage rules. This demo only shows retrieval + prompt composition and uses a placeholder for model calls.\n",
    "\n",
    "---\n",
    "\n",
    "Below are the code cells in order. Execute them sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample documents (small)\n",
    "documents = [\n",
    "    \"The Port of Hamburg is one of the largest ports in Europe and handles millions of TEU annually.\",\n",
    "    \"Container throughput can be forecasted using time-series models and operational features like arrival rates and berth availability.\",\n",
    "    \"Predictive maintenance for cranes reduces downtime by combining sensor data with failure logs and scheduled inspections.\",\n",
    "    \"RAG systems use dense vector retrieval (embeddings) to fetch supporting documents, then condition an LLM on the retrieved context.\"\n",
    "]\n",
    "\n",
    "len(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0e895b",
   "metadata": {},
   "source": [
    "\n",
    "**Install required packages** (run in a code cell if needed):\n",
    "\n",
    "```bash\n",
    "pip install -U sentence-transformers faiss-cpu numpy\n",
    "```\n",
    "If you're in Colab, use:\n",
    "```python\n",
    "!pip install -U sentence-transformers faiss-cpu numpy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba42bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create embeddings using sentence-transformers\n",
    "# Note: If running here, ensure sentence-transformers is installed.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # small, fast model\n",
    "embeddings = model.encode(documents, convert_to_numpy=True)\n",
    "print('Embeddings shape:', embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c47ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build a FAISS index (CPU)\n",
    "import faiss\n",
    "\n",
    "d = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings)\n",
    "print('Indexed vectors:', index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3dd300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple retrieval function\n",
    "def retrieve(query, k=2):\n",
    "    q_emb = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(q_emb, k)\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        results.append({'doc': documents[idx], 'score': float(dist)})\n",
    "    return results\n",
    "\n",
    "# Example query\n",
    "query = 'How can we predict container throughput?'\n",
    "results = retrieve(query, k=2)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63954ba3",
   "metadata": {},
   "source": [
    "\n",
    "## Prompt composition (example)\n",
    "Compose a prompt for an LLM by combining the retrieved context with the user question.\n",
    "\n",
    "**Example template (to be sent to an LLM):**\n",
    "```\n",
    "You are an expert on port logistics. Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "[retrieved_doc_1]\n",
    "[retrieved_doc_2]\n",
    "...\n",
    "\n",
    "Question:\n",
    "[User question]\n",
    "\n",
    "Answer concisely and include which assumptions you make.\n",
    "```\n",
    "\n",
    "Below is an example of how to create that prompt in code (no API call shown).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3cc431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the prompt from retrieved docs\n",
    "retrieved = retrieve('What methods to forecast arrivals in a port?', k=2)\n",
    "context = '\\n\\n'.join([r['doc'] for r in retrieved])\n",
    "user_question = 'Welche Methoden eignen sich zur Vorhersage von Ankunftszeiten im Hafen?'\n",
    "\n",
    "prompt = f\"\"\"You are an expert on port logistics. Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{user_question}\n",
    "\n",
    "Answer concisely in German and mention key features you would use for forecasting.\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104f910",
   "metadata": {},
   "source": [
    "\n",
    "## Next steps / notes\n",
    "- To run a full RAG pipeline in production, consider:\n",
    "  - Storing embeddings in a managed vector DB (Pinecone, Milvus, Weaviate) or a scalable FAISS setup.\n",
    "  - Using a model registry (e.g., MLflow) for your embedding models and LLM prompt templates.\n",
    "  - Implementing security, access control, and data governance (DSGVO) for documents used in retrieval.\n",
    "- This notebook is intentionally minimal so you can quickly run and adapt it. Add real documents, increase k, or plug an LLM provider for generation.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
