{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5127d25",
   "metadata": {},
   "source": [
    "# RAG + MLflow Demo Notebook\n",
    "\n",
    "Dieses Notebook demonstriert:\n",
    "- Erzeugen von Embeddings mit `sentence-transformers`\n",
    "- Indexieren mit `faiss`\n",
    "- Retrieval für eine Anfrage\n",
    "- Tracking von Embedding-Modell, Embeddings und FAISS-Index mit `mlflow`\n",
    "\n",
    "Ausführungsanleitung:\n",
    "1. Installiere Abhängigkeiten: `pip install -r requirements.txt`\n",
    "2. Starte das Notebook und führe die Zellen nacheinander aus.\n",
    "3. Optional: `mlflow ui --backend-store-uri ./mlruns` um Runs zu inspizieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f6542",
   "metadata": {},
   "source": [
    "## 1) Imports und Beispieldokumente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b517f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Beispiel-Dokumente\n",
    "documents = [\n",
    "    \"The Port of Hamburg is one of the largest ports in Europe and handles millions of TEU annually.\",\n",
    "    \"Container throughput can be forecasted using time-series models and operational features like arrival rates and berth availability.\",\n",
    "    \"Predictive maintenance for cranes reduces downtime by combining sensor data with failure logs and scheduled inspections.\",\n",
    "    \"RAG systems use dense vector retrieval (embeddings) to fetch supporting documents, then condition an LLM on the retrieved context.\"\n",
    "]\n",
    "len(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26769463",
   "metadata": {},
   "source": [
    "## 2) Install Hinweis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef754cdd",
   "metadata": {},
   "source": [
    "Install required packages: `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb86685",
   "metadata": {},
   "source": [
    "## 3) Embeddings erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4b97a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(documents, convert_to_numpy=True)\n",
    "print('Embeddings shape:', embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de242e7c",
   "metadata": {},
   "source": [
    "## 4) FAISS Index erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22195857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import faiss\n",
    "d = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings)\n",
    "print('Indexed vectors:', index.ntotal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9342db5",
   "metadata": {},
   "source": [
    "## 5) Retrieval-Funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve(query, k=2):\n",
    "    q_emb = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(q_emb, k)\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        results.append({'doc': documents[idx], 'score': float(dist)})\n",
    "    return results\n",
    "\n",
    "retrieve('How to forecast container throughput?', k=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3309587",
   "metadata": {},
   "source": [
    "## 6) MLflow Integration — lokal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c577751",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MLflow Beispiel: lokal tracken (./mlruns)\n",
    "import mlflow\n",
    "import json\n",
    "import faiss\n",
    "\n",
    "mlflow.set_tracking_uri('file://' + './mlruns')\n",
    "with mlflow.start_run(run_name='notebook_rag_demo'):\n",
    "    mlflow.log_param('embedding_model', 'all-MiniLM-L6-v2')\n",
    "    # Speichere Dokumente\n",
    "    with open('documents.json','w',encoding='utf-8') as f:\n",
    "        json.dump(documents, f, ensure_ascii=False, indent=2)\n",
    "    mlflow.log_artifact('documents.json', artifact_path='rag_docs')\n",
    "    # Embeddings als numpy speichern und loggen\n",
    "    np.save('embeddings.npy', embeddings)\n",
    "    mlflow.log_artifact('embeddings.npy', artifact_path='embeddings')\n",
    "    # FAISS Index speichern\n",
    "    faiss.write_index(index, 'faiss.index')\n",
    "    mlflow.log_artifact('faiss.index', artifact_path='faiss_index')\n",
    "    # Beispiel-Retrieval loggen\n",
    "    query = 'Welche Methoden eignen sich zur Vorhersage von Ankunftszeiten im Hafen?'\n",
    "    results = retrieve(query, k=2)\n",
    "    with open('retrieval.json','w',encoding='utf-8') as f:\n",
    "        json.dump({'query': query, 'results': results}, f, ensure_ascii=False, indent=2)\n",
    "    mlflow.log_artifact('retrieval.json', artifact_path='retrievals')\n",
    "    # Log distances as metrics\n",
    "    for i, r in enumerate(results):\n",
    "        mlflow.log_metric(f'dist_{i}', r['score'])\n",
    "    print('MLflow run logged — öffne mlflow ui mit: mlflow ui --backend-store-uri ./mlruns')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f7ab4",
   "metadata": {},
   "source": [
    "## 7) Prompt composition (ohne LLM-Aufruf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retrieved = retrieve('What methods to forecast arrivals in a port?', k=2)\n",
    "context = '\\n\\n'.join([r['doc'] for r in retrieved])\n",
    "user_question = 'Welche Methoden eignen sich zur Vorhersage von Ankunftszeiten im Hafen?'\n",
    "\n",
    "prompt = f\"\"\"You are an expert on port logistics. Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{user_question}\n",
    "\n",
    "Answer concisely in German and mention key features you would use for forecasting.\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
